---
layout: post
comments: true
mathjax: true
priority: 900
title: “Generative models”
excerpt: “Generative models”
date: 2017-03-06 14:00:00
---

### Discriminative models

In a discriminative model, we draw conclusion on something we observe. We can use a CNN to classify an image (the observation). For example, we train a discriminative model to classify whether an image is a bedroom, a bathroom or a kitchen etc... 

$$ y = f(image) $$

<div class="imgcap">
<img src="/assets/cnn/cnn.png" style="border:none;width:60%">
</div>

#### Class visualization

Often, we want to know what contributes to a bedroom. We want to know what features a network tries to learn in the classification process. First, we train a classification model for example using a CNN. Then we genearte a random image and feed forward to the network. Instead of backpropagate the gradient to train $$W$$, we backpropagate the gradient to make $$image$$ to look like the target class. i.e., use backpropagation to change the $$image$$ (instead of $$W$$) to increase the score of the target class. In order to do so, we change $$ \frac{\partial J}{\partial score_i} $$ manually to:

$$
\frac{\partial J}{\partial score_i}=
    \left\{
    \begin{array}{lr}
      1,& i=target \\
      0,& i \neq target 
    \end{array}
    \right\}
$$

, and reiterate the feed forward and backward many times. Here is the skeleton code to generate an image from a pre-trained CNN _model_ for the target class _target_y_.

```phtyon
def class_visualization(target_y, model, learning_rate, l2_reg, num_iterations):

    # Generate a random image
    X = np.random.randn(1, 3, 64, 64)
    for t in xrange(num_iterations):
        dX = None
        scores, cache = model.forward(X, mode='test')

        # Artifically set the dscores for our target to 1, otherwise 0.
        dscores = np.zeros_like(scores)
        dscores[0, target_y] = 1.0

        # Backpropagate
        dX, grads = model.backward(dscores, cache)
        dX -= 2 * l2_reg * X
		
        # Change the image with the gradient descent.
        X += learning_rate * dX
    return X
```

To make it works better, we add clipping, jittering and blurring:
```phtyon
def class_visualization(target_y, model, learning_rate, l2_reg, num_iterations, blur_every, max_jitter):
    X = np.random.randn(1, 3, 64, 64)
    for t in xrange(num_iterations):
        # Add the jitter
        ox, oy = np.random.randint(-max_jitter, max_jitter + 1, 2)
        X = np.roll(np.roll(X, ox, -1), oy, -2)

        dX = None
        scores, cache = model.forward(X, mode='test')

        dscores = np.zeros_like(scores)
        dscores[0, target_y] = 1.0
        dX, grads = model.backward(dscores, cache)
        dX -= 2 * l2_reg * X

        X += learning_rate * dX

        # Undo the jitter
        X = np.roll(np.roll(X, -ox, -1), -oy, -2)

        # As a regularizer, clip the image
        X = np.clip(X, -data['mean_image'], 255.0 - data['mean_image'])

        # As a regularizer, periodically blur the image
        if t % blur_every == 0:
            X = blur_image(X)
    return X
```


Here is a spider image generated by the code:
<div class="imgcap">
<img src="/assets/gm/spider.png" style="border:none;width:30%">
</div>


#### Google DeepDream

Google DeepDream uses a CNN to find and enhance features within an image. It forward feed an image to a CNN network to extract features at a particular layer. It later backpropagate the gradient by explicitly changing the gradient to its activation:

$$ 
\frac{\partial J}{\partial score_i} = activation
$$ 

This exaggerate the features at the chosen layer of the network.

```python
out, cache = model.forward(X, end=layer)
dX, grads = model.backward(out, cache)
X += learning_rate * dX
```

Here, we start with an image of a cat, and this is the image after many iterations:
<div class="imgcap">
<img src="/assets/gm/cat.png" style="border:none;width:40%">
</div>

Original:
<div class="imgcap">
<img src="/assets/gm/cat2.jpg" style="border:none;width:40%">
</div>

> We turn around a CNN network to generate realistic images through backpropagation.

### Generative models

In a discriminative model, we draw conclusion on something we observe: 

$$ y = f(image) $$

A generative model generates data that we observe:

$$ image = G(z) $$

For example, in a generative model, we ask the model to generate an image that resemble a bedroom. In previous sections, we train a CNN to extract features of our training dataset. Then we iteratively and selectively backpropagate features to generate images. In the following sections, we adopt a more direct approach in generating image.

> z can sometimes realize as the latent variables of an image.

#### DCGAN (Deep Convolutional Generative Adversarial Networks)
In DCGAN, we generate an image directly using a deep network while using a second discriminator network to guide the generation process. Here is the generator network:

Source - Alec Radford, Luke Metz, Soumith Chintala:
<div class="imgcap">
<img src="/assets/gm/gm.png" style="border:none;width:80%">
</div>

$$ image = G(z) $$

The input $$ z $$ to the model is a 100-Dimensional vector (100 random numbers). We randomly select input vectors and create images $$ z_{out} $$ using multiple layers of transpose convolutions  (CONV 1, ... CONV 4).

The following is a demonstration of using a transposed convolution to generate a 4x4 region from a 2x2 region using a 3x3 filter. [(Animation source)](https://github.com/vdumoulin/conv_arithmetic):
<div class="imgcap">
<img src="/assets/gm/transpose.gif" style="border:none;width:30%">
</div>

> Transpose convolutions are sometimes call deconvolution.

At the beginning, $$ z_{out} $$ are just random noisy images. In DCGAN, we use a second network called a discriminator to guide how images are generated. With the training dataset and the generated images from the generator network, we train the discriminator (just another CNN classifier) to classify whether its input image is real or generated. But simultaneously, for generated images, we backpropagation the score to the generator network.  The purpose is to train the $$W$$ of the generator network so it can generate more realistic images. So the discriminator servers 2 purposes. It determines the fake one from the real one and trains the generative model to create better images through backpropagation. By training both networks simultaneously, the discriminator is better in distinguish generated images while the generator is better in make better images with the help of the discriminator. As both improving, the gap between the real and generated one will be diminished.

The following are some room pictures generated by a generative model: 

Source - Alec Radford, Luke Metz, Soumith Chintala:
<div class="imgcap">
<img src="/assets/gm/room.png" style="border:none;width:60%">
</div>

As we change $$ z $$ gradually, the images will be changed gradually also.

<div class="imgcap">
<img src="/assets/gm/rm.png" style="border:none;width:60%">
</div>

> The following code is written in TensorFlow. 

Here is a discriminator network that looks similar to the usual CNN classification network. It compose of 4 convolution layers. With the exception of the first convolution layer, other convolution layers are linked with a batch normalization layer and then a leaky ReLU. Finally, it is connected to a fully connected layer (linear) with a sigmoid classifier.
```python
def discriminator(image):
    d_bn1 = batch_norm(name='d_bn1')
    d_bn2 = batch_norm(name='d_bn2')
    d_bn3 = batch_norm(name='d_bn3')

    h0 = lrelu(conv2d(image, DIM, name='d_h0'))
    h1 = lrelu(d_bn1(conv2d(h0, DIM * 2, name='d_h1')))
    h2 = lrelu(d_bn2(conv2d(h1, DIM * 4, name='d_h2')))
    h3 = lrelu(d_bn3(conv2d(h2, DIM * 8, name='d_h3')))
    h4 = linear(tf.reshape(h3, [batchsize, -1]), 1, scope='d_h4')
    return tf.nn.sigmoid(h4), h4
```

The generator looks very similar to the reverse of the discriminator except the convolution layer is replaced with the transpose convolution layer.
```python
def generator(z):
    g_bn0 = batch_norm(name='g_bn0')
    g_bn1 = batch_norm(name='g_bn1')
    g_bn2 = batch_norm(name='g_bn2')
    g_bn3 = batch_norm(name='g_bn3')

    z2 = linear(z, DIM * 8 * 4 * 4, scope='g_h0')
    h0 = tf.nn.relu(g_bn0(tf.reshape(z2, [-1, 4, 4, DIM * 8])))
    h1 = tf.nn.relu(g_bn1(conv_transpose(h0, [batchsize, 8, 8, DIM * 4], name="g_h1")))
    h2 = tf.nn.relu(g_bn2(conv_transpose(h1, [batchsize, 16, 16, DIM * 2], name="g_h2")))
    h3 = tf.nn.relu(g_bn3(conv_transpose(h2, [batchsize, 32, 32, DIM * 1], name="g_h3")))
    h4 = conv_transpose(h3, [batchsize, 64, 64, 3], name="g_h4")
    return tf.nn.tanh(h4)
```

We build a placeholder for image to the discriminator, and a placeholder for $$z$$. We build 1 generator and 2 discriminator that shared all the tunable parameters: one for real image and one for generated image.
```python
    images = tf.placeholder(tf.float32, [batchsize, DIM, DIM, 3] , name="real_images")
    zin = tf.placeholder(tf.float32, [None, Z_DIM], name="z")

    G = generator(zin)
    with tf.variable_scope("discriminator") as scope:
        D_prob, D_logit = discriminator(images)
        scope.reuse_variables()
        D_fake_prob, D_fake_logit = discriminator(G)
```

We computed the lost function for the discriminator.
```python
    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit, labels=tf.ones_like(D_logit)))
    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logit, labels=tf.zeros_like(D_fake_logit)))
    d_loss = d_loss_real + d_loss_fake
```

We compute the lost function for the generator by using the logits from the discriminator. Hence, we train the $$W$$ in the generator to create more realistic images.
```python
    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logit, labels=tf.ones_like(D_fake_logit)))
```

We use 2 separate optimizer to train both network simultaneously:
```python
    t_vars = tf.trainable_variables()
    d_vars = [var for var in t_vars if 'd_' in var.name]
    g_vars = [var for var in t_vars if 'g_' in var.name]

    d_optim = tf.train.AdamOptimizer(learningrate, beta1=beta1).minimize(d_loss, var_list=d_vars)
    g_optim = tf.train.AdamOptimizer(learningrate, beta1=beta1).minimize(g_loss, var_list=g_vars)
```

The full source code is available [here](https://github.com/jhui/machine_learning/tree/master/generative_adversarial_network).
 
### Credits
Part of the source code for GAN is originated from https://github.com/kvfrans/generative-adversial and https://github.com/carpedm20/DCGAN-tensorflow.